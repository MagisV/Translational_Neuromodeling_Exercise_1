{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.1.a\n",
    "\n",
    "### Step 1: Probability distribution of y\n",
    "As $\\varepsilon$ follows the normal distribution, the conditional distribution of y given x is:\n",
    "\n",
    "$$\n",
    "y \\mid x \\sim \\mathcal{N} \\left( \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P, \\sigma^2 \\right)\n",
    "$$\n",
    "\n",
    "The probability density function (PDF) for a normal distribution is:\n",
    "\n",
    "$$\n",
    "p(y \\mid x, \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y - \\mu(x))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Where $\\mu(x)$ is the predicted value:\n",
    "\n",
    "$$\n",
    "\\mu(x) = \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Likelihood function\n",
    "\n",
    "Given N independent observations $\\mathbf{y} = (y_1, y_2, \\dots, y_N)^T$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} p(y_i \\mid x_i, \\theta, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Substituting the normal PDF:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Log-likelihood function\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = -\\frac{N}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\mu(x_i))^2\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\mu(x_i) = \\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P.\n",
    "$$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum likelihood estimates, we differentiate the log-likelihood with respect to each parameter $\\theta_j$ and set the derivative equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\log L(\\theta, \\sigma^2) = 0\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\left[ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i))^2 \\right] = 0\n",
    "$$\n",
    "\n",
    "Carrying out this differentiation explicitly, we get:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0\n",
    "$$\n",
    "\n",
    "Since  $\\sigma^2$ is positive and nonzero, we simplify to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0 \\quad \\text{for each } j=0,1,...,P\n",
    "$$\n",
    "\n",
    "This results in the linear system of equations (normal equations):\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\left(y_i - (\\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P)\\right)x_i^j = 0 \\quad \\text{for each } j=0,1,\\dots,P.\n",
    "$$\n",
    "\n",
    "In matrix form, this solution can be expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where $X$ is the design matrix containing the polynomial terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "\n",
    "theta_0 = 0.3\n",
    "theta_1 = -0.1\n",
    "theta_2 = 0.5\n",
    "variance = 0.0001\n",
    "standard_deviation = sqrt(variance)\n",
    "\n",
    "function get_y_for_noisy_x(start, step, stop, seed)\n",
    "    x = start:step:stop\n",
    "    Random.seed!(seed)\n",
    "    epsilon = randn(length(x))\n",
    "    y = theta_0 .+ theta_1*x .+ theta_2*x.^2 .+ standard_deviation*epsilon\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "x, y = get_y_for_noisy_x(-0.5, 0.1, 0.2, 42)\n",
    "\n",
    "scatter(x, y, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: create design matrix for polynomial of order P\n",
    "function design_matrix(xvals, P)\n",
    "    # X will have columns [1, x, x^2, ..., x^P]\n",
    "    X = [xi^p for xi in xvals, p in 0:P]\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Compute ML estimate \\hat{theta} via normal equations and log-likelihood\n",
    "function ml_estimate(xvals, yvals, P)\n",
    "    X = design_matrix(xvals, P)\n",
    "    # ML parameter estimate using least squares\n",
    "    theta_hat = (X'X) \\ (X'yvals)\n",
    "\n",
    "    # Fitted values\n",
    "    y_hat = X * theta_hat\n",
    "\n",
    "    # Residual sum of squares (RSS)\n",
    "    rss = sum((yvals .- y_hat).^2)\n",
    "\n",
    "    # Estimate of sigma^2 is RSS / N in the ML setting\n",
    "    sigma2_hat = rss / length(yvals)\n",
    "\n",
    "    # Log-likelihood under Gaussian noise\n",
    "    # L = -N/2 * log(2πσ_hat^2) - RSS/(2σ_hat^2)\n",
    "    N = length(yvals)\n",
    "    logL = -N/2 * log(2*π*sigma2_hat) - (rss / (2*sigma2_hat))\n",
    "\n",
    "    return theta_hat, logL\n",
    "end\n",
    "\n",
    "# compute estimates and log-likelihoods for P=1, P=2, P=7\n",
    "P_values = [1, 2, 7]\n",
    "estimates = [ml_estimate(x, y, P) for P in P_values]\n",
    "\n",
    "# print the estimates and log-likelihoods for each P value\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    println(\"P = $P: \\n theta_hat = $theta_hat, \\n logL = $logL\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three parameters of $\\theta$ are closest to their true values when the number of parameters is the same as in the original function. The function with only 2 parameters is underfitting and the function with 7 parameters is overfitting.\n",
    "\n",
    "The log-likelihood is increasing with the number of parameters. This is an indicator that, for noisy data, if the data is too likely to have been generated by the model, the model is likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = get_y_for_noisy_x(-0.5, 0.01, 0.5, 42)\n",
    "\n",
    "plot(x_new, y_new, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the log-likelihood for the new data under the ML parameter estimates obtained in (d) for P = 1,2 and 7\n",
    "function log_likelihood_for_specific_params(xvals, yvals, theta_hat)\n",
    "    P = length(theta_hat) - 1\n",
    "    N = length(yvals)\n",
    "    # estimates\n",
    "    y_hat = design_matrix(xvals, P) * theta_hat\n",
    "\n",
    "    # RSS\n",
    "    rss = sum((yvals .- y_hat).^2)\n",
    "\n",
    "    # sigma^2\n",
    "    sigma2_hat = rss / N\n",
    "    \n",
    "    # log-likelihood\n",
    "    logL = N/2 * log(2*π*sigma2_hat) - (rss / (2*sigma2_hat))\n",
    "\n",
    "    return logL\n",
    "end\n",
    "\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    println(\"P = $P: \\n logL = $(log_likelihood_for_specific_params(x_new, y_new, theta_hat))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and each of the fitted polynomials, keep the y-axis at 0 - 1\n",
    "plot(x_new, y_new, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\", ylims=(0, 1))\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    y_hat = design_matrix(x_new, P) * theta_hat\n",
    "    plot!(x_new, y_hat, label=\"P = $P\")\n",
    "end\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
