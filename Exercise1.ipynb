{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.1.a\n",
    "\n",
    "### Step 1: Probability distribution of y\n",
    "As $\\varepsilon$ follows the normal distribution, the conditional distribution of y given x is:\n",
    "\n",
    "$$\n",
    "y \\mid x \\sim \\mathcal{N} \\left( \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P, \\sigma^2 \\right)\n",
    "$$\n",
    "\n",
    "The probability density function (PDF) for a normal distribution is:\n",
    "\n",
    "$$\n",
    "p(y \\mid x, \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y - \\mu(x))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Where $\\mu(x)$ is the predicted value:\n",
    "\n",
    "$$\n",
    "\\mu(x) = \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Likelihood function\n",
    "\n",
    "Given N independent observations $\\mathbf{y} = (y_1, y_2, \\dots, y_N)^T$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} p(y_i \\mid x_i, \\theta, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Substituting the normal PDF:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Log-likelihood function\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = -\\frac{N}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\mu(x_i))^2\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\mu(x_i) = \\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P.\n",
    "$$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum likelihood estimates, we differentiate the log-likelihood with respect to each parameter $\\theta_j$ and set the derivative equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\log L(\\theta, \\sigma^2) = 0\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\left[ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i))^2 \\right] = 0\n",
    "$$\n",
    "\n",
    "Carrying out this differentiation explicitly, we get:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0\n",
    "$$\n",
    "\n",
    "Since  $\\sigma^2$ is positive and nonzero, we simplify to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0 \\quad \\text{for each } j=0,1,...,P\n",
    "$$\n",
    "\n",
    "This results in the linear system of equations (normal equations):\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\left(y_i - (\\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P)\\right)x_i^j = 0 \\quad \\text{for each } j=0,1,\\dots,P.\n",
    "$$\n",
    "\n",
    "In matrix form, this solution can be expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where $X$ is the design matrix containing the polynomial terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "\n",
    "theta_0 = 0.3\n",
    "theta_1 = -0.1\n",
    "theta_2 = 0.5\n",
    "variance = 0.0001\n",
    "standard_deviation = sqrt(variance)\n",
    "\n",
    "x = -0.5:0.1:0.2\n",
    "\n",
    "Random.seed!(42) # seed for reproducibility\n",
    "epsilon = randn(length(x))\n",
    "y = theta_0 .+ theta_1*x .+ theta_2*x.^2 .+ standard_deviation*epsilon\n",
    "\n",
    "scatter(x, y, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: create design matrix for polynomial of order P\n",
    "function design_matrix(xvals, P)\n",
    "    # X will have columns [1, x, x^2, ..., x^P]\n",
    "    X = [xi^p for xi in xvals, p in 0:P]\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Compute ML estimate \\hat{theta} via normal equations and log-likelihood\n",
    "function ml_estimate(xvals, yvals, P)\n",
    "    X = design_matrix(xvals, P)\n",
    "    # ML parameter estimate using least squares\n",
    "    theta_hat = (X'X) \\ (X'yvals)\n",
    "\n",
    "    # Fitted values\n",
    "    y_hat = X * theta_hat\n",
    "\n",
    "    # Residual sum of squares (RSS)\n",
    "    rss = sum((yvals .- y_hat).^2)\n",
    "\n",
    "    # Estimate of sigma^2 is RSS / N in the ML setting\n",
    "    sigma2_hat = rss / length(yvals)\n",
    "\n",
    "    # Log-likelihood under Gaussian noise\n",
    "    # L = -N/2 * log(2πσ_hat^2) - RSS/(2σ_hat^2)\n",
    "    N = length(yvals)\n",
    "    logL = -N/2 * log(2*π*sigma2_hat) - (rss / (2*sigma2_hat))\n",
    "\n",
    "    return theta_hat, logL\n",
    "end\n",
    "\n",
    "# Compare parameter estimates and log-likelihood for P=1, P=2, P=7\n",
    "println(\"-------------------------------------------------\")\n",
    "println(\"True theta = [$theta_0, $theta_1, $theta_2]\")\n",
    "for p in (1, 2, 7)\n",
    "    θhat, logL = ml_estimate(x, y, p)\n",
    "    println(\"Polynomial order P = $p\")\n",
    "    println(\"  ML parameter estimates = \", θhat)\n",
    "    println(\"  Log-likelihood         = $logL\")\n",
    "    println(\"-------------------------------------------------\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three parameters of $\\theta$ are closest to their true values when the number of parameters is the same as in the original function. The function with only 2 parameters is underfitting and the function with 7 parameters is overfitting.\n",
    "\n",
    "The log-likelihood is increasing with the number of parameters. This is an indicator that, for noisy data, if the data is too likely to have been generated by the model, the model is likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
