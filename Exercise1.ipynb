{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "As $\\varepsilon$ follows the normal distribution, the conditional distribution of y given x is:\n",
    "\n",
    "$$\n",
    "y \\mid x \\sim \\mathcal{N} \\left( \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P, \\sigma^2 \\right)\n",
    "$$\n",
    "\n",
    "The probability density function (PDF) for a normal distribution is:\n",
    "\n",
    "$$\n",
    "p(y \\mid x, \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y - \\mu(x))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Where $\\mu(x)$ is the predicted value:\n",
    "\n",
    "$$\n",
    "\\mu(x) = \\theta_0 + \\theta_1 x + \\dots + \\theta_P x^P.\n",
    "$$\n",
    "\n",
    "Given N independent observations $\\mathbf{y} = (y_1, y_2, \\dots, y_N)^T$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} p(y_i \\mid x_i, \\theta, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Substituting the normal PDF:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\sigma^2) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta, \\sigma^2) = -\\frac{N}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\mu(x_i))^2\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\mu(x_i) = \\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum likelihood estimates, we differentiate the log-likelihood with respect to each parameter $\\theta_j$ and set the derivative equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\log L(\\theta, \\sigma^2) = 0\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\left[ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i))^2 \\right] = 0\n",
    "$$\n",
    "\n",
    "Carrying out this differentiation explicitly, we get:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0\n",
    "$$\n",
    "\n",
    "Since  $\\sigma^2$ is positive and nonzero, we simplify to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}(y_i - \\mu(x_i)) x_i^j = 0 \\quad \\text{for each } j=0,1,...,P\n",
    "$$\n",
    "\n",
    "This results in the linear system of equations (normal equations):\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\left(y_i - (\\theta_0 + \\theta_1 x_i + \\dots + \\theta_P x_i^P)\\right)x_i^j = 0 \\quad \\text{for each } j=0,1,\\dots,P.\n",
    "$$\n",
    "\n",
    "In matrix form, this solution can be expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where $X$ is the design matrix containing the polynomial terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Random\")\n",
    "using Random\n",
    "using Plots\n",
    "\n",
    "theta_0 = 0.3\n",
    "theta_1 = -0.1\n",
    "theta_2 = 0.5\n",
    "variance = 0.0001\n",
    "standard_deviation = sqrt(variance)\n",
    "\n",
    "function get_y_for_noisy_x(start, step, stop, seed)\n",
    "    x = start:step:stop\n",
    "    Random.seed!(seed)\n",
    "    epsilon = randn(length(x))\n",
    "    y = theta_0 .+ theta_1*x .+ theta_2*x.^2 .+ standard_deviation*epsilon\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "x, y = get_y_for_noisy_x(-0.5, 0.1, 0.2, 42)\n",
    "\n",
    "scatter(x, y, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: create design matrix for polynomial of order P\n",
    "function design_matrix(xvals, P)\n",
    "    # X will have columns [1, x, x^2, ..., x^P]\n",
    "    X = [xi^p for xi in xvals, p in 0:P]\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Compute ML estimate \\hat{theta} via normal equations and log-likelihood\n",
    "function ml_estimate(xvals, yvals, P)\n",
    "    X = design_matrix(xvals, P)\n",
    "    # ML parameter estimate using least squares\n",
    "    theta_hat = (X'X) \\ (X'yvals)\n",
    "\n",
    "    # Fitted values\n",
    "    y_hat = X * theta_hat\n",
    "\n",
    "    # Residual sum of squares (RSS)\n",
    "    rss = sum((yvals .- y_hat).^2)\n",
    "\n",
    "    # Estimate of sigma^2 is RSS / N in the ML setting\n",
    "    sigma2_hat = rss / length(yvals)\n",
    "\n",
    "    # Log-likelihood under Gaussian noise\n",
    "    # L = -N/2 * log(2πσ_hat^2) - RSS/(2σ_hat^2)\n",
    "    N = length(yvals)\n",
    "    logL = -N/2 * log(2*π*sigma2_hat) - (rss / (2*sigma2_hat))\n",
    "\n",
    "    return theta_hat, logL\n",
    "end\n",
    "\n",
    "# compute estimates and log-likelihoods for P=1, P=2, P=7\n",
    "P_values = [1, 2, 7]\n",
    "estimates = [ml_estimate(x, y, P) for P in P_values]\n",
    "\n",
    "# print the estimates and log-likelihoods for each P value\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    println(\"P = $P: \\n theta_hat = $theta_hat, \\n logL = $logL\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three parameters of $\\theta$ are closest to their true values when the number of parameters is the same as in the original function. The function with only 2 parameters is underfitting and the function with 7 parameters is overfitting.\n",
    "\n",
    "The log-likelihood is increasing with the number of parameters. This is an indicator that, for noisy data, if the data is too likely to have been generated by the model, the model is likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = get_y_for_noisy_x(-0.5, 0.01, 0.5, 42)\n",
    "\n",
    "plot(x_new, y_new, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the log-likelihood for the new data under the ML parameter estimates obtained in (d) for P = 1,2 and 7\n",
    "function log_likelihood_for_specific_params(xvals, yvals, theta_hat)\n",
    "    P = length(theta_hat) - 1\n",
    "    N = length(yvals)\n",
    "    # estimates\n",
    "    y_hat = design_matrix(xvals, P) * theta_hat\n",
    "\n",
    "    # RSS\n",
    "    rss = sum((yvals .- y_hat).^2)\n",
    "\n",
    "    # sigma^2\n",
    "    sigma2_hat = rss / N\n",
    "    \n",
    "    # log-likelihood\n",
    "    logL = -N/2 * log(2*π*sigma2_hat) - (rss / (2*sigma2_hat))\n",
    "\n",
    "    return logL\n",
    "end\n",
    "\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    println(\"P = $P: \\n logL = $(log_likelihood_for_specific_params(x_new, y_new, theta_hat))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the previous parameters to a wider range on the x-axis and a smaller step size one can observe the generelisability of the different models. The model with P=2 is not by far the most likely to have generated the data. P=7, which fit the noise quite well on a shorter scale is now a fairly bad model because outside the range of the previous data it deviates significantly from the original quadratic function. \n",
    "\n",
    "Plotting the graphs of the different fits and the newly generated data this can be seen fairly easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and each of the fitted polynomials, keep the y-axis at 0 - 1\n",
    "plot(x_new, y_new, label=\"data\", xlabel=\"x\", ylabel=\"y\", title=\"Data\", ylims=(0, 1))\n",
    "for (P, (theta_hat, logL)) in zip(P_values, estimates)\n",
    "    y_hat = design_matrix(x_new, P) * theta_hat\n",
    "    plot!(x_new, y_hat, label=\"P = $P\")\n",
    "end\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "N = 100\n",
    "pvals = [1, 2, 7]\n",
    "theta_estimates = Dict(p => [] for p in pvals)\n",
    "\n",
    "# 1) Generate data & collect ML-estimated parameters over repeated runs\n",
    "for i in 1:N\n",
    "    # Get a new dataset\n",
    "    x, y = get_y_for_noisy_x(-0.5, 0.1, 0.2, i)\n",
    "    \n",
    "    # Fit each polynomial degree and store the parameter vector\n",
    "    for p in pvals\n",
    "        θ̂ = ml_estimate(x, y, p)[1]  # returns a vector of length (p+1)\n",
    "        push!(theta_estimates[p], θ̂)\n",
    "    end\n",
    "end\n",
    "\n",
    "# 2) For each polynomial order p, flatten ALL parameters into one big vector\n",
    "#    and draw a single histogram of these values.\n",
    "for p in pvals\n",
    "    # theta_estimates[p] is a Vector-of-Vectors:\n",
    "    #   [ [θ̂_0, θ̂_1, ...], [θ̂_0, θ̂_1, ...], ... ]\n",
    "    # We'll flatten (vcat) them to one 1D array of length N*(p+1).\n",
    "    all_params = vcat(theta_estimates[p]...)  \n",
    "\n",
    "    histogram(\n",
    "        all_params,\n",
    "        bins=200,\n",
    "        title=\"Histogram of all parameters for p = $p (N=$N runs)\",\n",
    "        xlabel=\"Parameter values\",\n",
    "        ylabel=\"Frequency\"\n",
    "    )\n",
    "    display(current())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P=1 and P=2 show quite clearly the distributions for their parameters. P=7 has a very wide range of parameter values and mostly no clear collection of parameter values that belong to a specific parameter. Let's look at the distribution of each parameter separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = length(pvals)\n",
    "ncols = maximum(pvals) + 1  # because p=7 => 8 parameters at most\n",
    "plt = plot(layout=(nrows, ncols), size=(3000, 800))\n",
    "\n",
    "for (row_idx, p) in enumerate(pvals)\n",
    "    # parameter vectors for all runs, e.g. p+1 = number of parameters\n",
    "    runs = theta_estimates[p]\n",
    "    for j in 1:(p+1)\n",
    "        # Column index j for the subplot\n",
    "        col_idx = j  \n",
    "        # Extract j-th parameter from each run\n",
    "        param_vals = [runs[i][j] for i in 1:N]\n",
    "\n",
    "        # Choose which subplot we draw into\n",
    "        plot_idx = (row_idx - 1) * ncols + j\n",
    "        histogram!(\n",
    "            param_vals,\n",
    "            subplot=plot_idx,\n",
    "            bins=30,\n",
    "            title=\"p=$p, θ̂_$(j-1)\",  # j-1 so indexing matches usual 0..p\n",
    "            xlabel=\"Param value\",\n",
    "            ylabel=\"Frequency\"\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the number of parameters gets, the less consistent the parameters are. E.g. y12 and y13 have a range of about 5000 while y3 and y4 have a range < 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes' theorem, the posterior distribution of the parameters is given by:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta} | \\mathbf{y}) = \\frac{p(\\mathbf{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\mathbf{y})}\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\theta} | \\mathbf{y}) = \\log p(\\mathbf{y} | \\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta}) - \\log p(\\mathbf{y})\n",
    "$$\n",
    "\n",
    "Since $p(\\mathbf{y})$ is independent of $\\boldsymbol{\\theta}$, it can be ignored in optimization.\n",
    "\n",
    "From **Exercise 1.1**, the likelihood function assuming **iid Gaussian noise** is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\boldsymbol{\\theta}) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}} \\exp \\left( -\\frac{1}{2\\sigma^2} \\|\\mathbf{y} - X\\boldsymbol{\\theta}\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{y} | \\boldsymbol{\\theta}) = -\\frac{N}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - X\\boldsymbol{\\theta}\\|^2\n",
    "$$\n",
    "\n",
    "The prior is given as a **Gaussian distribution**:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{|2\\pi\\Sigma_0|}} \\exp \\left( -\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^T \\Sigma_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Since we assume **$\\Sigma_0 = I$ and $\\boldsymbol{\\mu}_0 = 0$**, the prior simplifies to:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{(2\\pi)^P}} \\exp \\left( -\\frac{1}{2} \\|\\boldsymbol{\\theta}\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\theta}) = -\\frac{P}{2} \\log(2\\pi) - \\frac{1}{2} \\|\\boldsymbol{\\theta}\\|^2\n",
    "$$\n",
    "\n",
    "Combining both terms:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\theta} | \\mathbf{y}) = -\\frac{N}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - X\\boldsymbol{\\theta}\\|^2 - \\frac{P}{2} \\log(2\\pi) - \\frac{1}{2} \\|\\boldsymbol{\\theta}\\|^2\n",
    "$$\n",
    "\n",
    "Ignoring constants that do not depend on $\\boldsymbol{\\theta}$, we obtain the **final expression for the log-posterior**:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\theta} | \\mathbf{y}) = -\\frac{1}{2\\sigma^2} \\|\\mathbf{y} - X\\boldsymbol{\\theta}\\|^2 - \\frac{1}{2} \\|\\boldsymbol{\\theta}\\|^2 + C\n",
    "$$\n",
    "\n",
    "where $C$ represents constants that do not affect optimization.\n",
    "\n",
    "This **log-posterior function** is the objective function for **Maximum-A-Posteriori (MAP) estimation**, which balances **data fitting (likelihood)** and **regularization (prior)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Optim\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "using Optim, LinearAlgebra\n",
    "\n",
    "# Define the log-likeliohood function\n",
    "function log_likelihood(theta, X, y, sigma_2)\n",
    "    N = length(y)\n",
    "    residual = y - X * theta\n",
    "    return - (1 / (2 * sigma_2)) * sum(residual .^ 2)\n",
    "end\n",
    "\n",
    "# Define the log-prior function\n",
    "function log_prior(theta)\n",
    "    return - (1 / 2) * sum(theta .^ 2)\n",
    "end\n",
    "\n",
    "# Define the negative log-posterior (to minimize)\n",
    "function neg_log_posterior(theta, X, y, sigma_2)\n",
    "    return - (log_likelihood(theta, X, y, sigma_2) + log_prior(theta))\n",
    "end\n",
    "\n",
    "# Function to compute the MAP estimate\n",
    "function compute_map_estimate(X, y, sigma_2)\n",
    "    P = size(X, 2)\n",
    "    theta_init = zeros(P)\n",
    "    result = optimize(theta -> neg_log_posterior(theta, X, y, sigma_2), theta_init, LBFGS())\n",
    "    return Optim.minimizer(result)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# Define the true parameters and sigma^2\n",
    "theta_true = [0.3, -0.1, 0.5]\n",
    "sigma_2 = 0.001\n",
    "\n",
    "# Generate the X and corresponding y values\n",
    "X_values = -0.5:0.01:0.2\n",
    "N = length(X_values)\n",
    "X = hcat(ones(N), X_values, X_values .^ 2)\n",
    "\n",
    "# Add Gaussian noise to the true model to generate y values\n",
    "y = X * theta_true + sqrt(sigma_2) * randn(N)\n",
    "\n",
    "# Apply the MAP estimator for different values of P\n",
    "P_values = [1, 2, 7]\n",
    "\n",
    "for P in P_values\n",
    "    X_P = hcat(ones(N), [X_values .^ p for p in 1:P]...) # Design matrix for Pth-order model\n",
    "    map_estimate = compute_map_estimate(X_P, y, sigma_2)\n",
    "    println(\"P = $P: MAP estimate = $map_estimate\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $P=1$ and $P=2$, the MAP estimates will be slightly different from the true values because the regularization slightly penalizes large values of $\\theta_1, \\theta_2$ etc. However, because the model is not too complex, the MAP estimates should still be reasonably close to the true values. The regularization effect will have a more noticeable impact if we have a larger value of P.\n",
    "\n",
    "For $P=7$, the MAP estimates should be closer to the true values compared to the ML estimates because the regularization imposed by the prior forces the parameters to remain more reasonable, rather than overfitting the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "# Number of repetitions\n",
    "reps = 100\n",
    "\n",
    "map_estimates_P1 = Float64[]\n",
    "map_estimates_P2 = Float64[]\n",
    "map_estimates_P7 = Float64[]\n",
    "\n",
    "for _ in 1:reps\n",
    "    y = X * theta_true + sqrt(sigma_2) * randn(N)\n",
    "\n",
    "    # Apply the MAP estimator for different values of P\n",
    "    X_P1 = hcat(ones(N), X_values)\n",
    "    X_P2 = hcat(ones(N), X_values, X_values .^ 2)\n",
    "    X_P7 = hcat(ones(N), [X_values .^ p for p in 1:7]...)\n",
    "\n",
    "    # MAP estimates for P = 1, P = 2, P = 7\n",
    "    map_estimate_P1 = compute_map_estimate(X_P1, y, sigma_2)\n",
    "    map_estimate_P2 = compute_map_estimate(X_P2, y, sigma_2)\n",
    "    map_estimate_P7 = compute_map_estimate(X_P7, y, sigma_2)\n",
    "\n",
    "    # Store the estimates\n",
    "    push!(map_estimates_P1, map_estimate_P1[2])\n",
    "    push!(map_estimates_P2, map_estimate_P2[2])\n",
    "    push!(map_estimates_P7, map_estimate_P7[2])\n",
    "end\n",
    "\n",
    "# Plot the histograms of the MAP estimates for each value of P\n",
    "histogram(map_estimates_P1, title=\"MAP Estimates for P=1\", xlabel=\"θ1\", ylabel=\"Frequency\")\n",
    "histogram(map_estimates_P2, title=\"MAP Estimates for P=2\", xlabel=\"θ1\", ylabel=\"Frequency\")\n",
    "histogram(map_estimates_P7, title=\"MAP Estimates for P=7\", xlabel=\"θ1\", ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP histograms will be narrower than the ML histograms, especially for higher-order models. This is due to the shrinkage imposed by the prior, which regularizes the estimates and prevents them from becoming too large.\n",
    "\n",
    "For higher-order models (like $P=7$), the MAP histograms will have less variance compared to the ML histograms. This is because the MAP estimation reduces overfitting by penalizing large parameter values through the prior.\n",
    "\n",
    "The MAP estimates will generally be closer to the true values of the parameters, particularly for higher-order models. The histograms for MAP will be tightly centered around the true values, especially for $P=7$, while the ML histograms might show more deviation due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
